{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhKYDydTfwkgDaabtQIvMA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekpenyong-Esu/Deep-RL-Exercise/blob/main/frozen_lake_real_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gym-notebook-wrapper\n",
        "\n",
        "#!sudo apt-get install xvfb"
      ],
      "metadata": {
        "id": "IsNafFctRXmk"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pygame"
      ],
      "metadata": {
        "id": "S4E14oU6nPXZ"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt-get install x11-utils > /dev/null 2>&1 \n",
        "#!pip install pyglet > /dev/null 2>&1 \n",
        "#!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "SKG59lmZcAEa"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "GGcMQp7Gk21P"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "from IPython.display import clear_output\n",
        "import time  # slow the game down a little bit\n",
        "import gym\n",
        "import numpy as np  # used for all kinds of matrix / vector operations\n",
        "import matplotlib.pyplot as plt  # for plotting\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import gnwrapper\n",
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "metadata": {
        "id": "G_T4Fcl6R90F"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode='human', is_slippery=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVavdPbFmkC3",
        "outputId": "38b0896e-cdda-4c9e-889a-839050947ca9"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Environment\n",
        "#env = gnwrapper.Animation(gym.make('FrozenLake-v1', is_slippery=False))"
      ],
      "metadata": {
        "id": "FS4dloVllzLV"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "\n",
        "state_size\n",
        "action_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AGlagXRnX34",
        "outputId": "ef4abf8d-1e3e-415c-fb02-ecd3b72cdbeb"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros([state_size, action_size])"
      ],
      "metadata": {
        "id": "q0mpoGjmXDWR"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDtyp1QkncKA",
        "outputId": "467c1abc-2c58-4a41-c700-b884abd07e7d"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtFDngQrnhey",
        "outputId": "67897ca8-dd1f-4879-c576-dc9ca429c339"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It is common to leave Hyperparameters in ALL CAPS to easily locate them\n",
        "\n",
        "EPOCHS=20000  # number of epochs/episodes to train for\n",
        "ALPHA = 0.8 # aka the learning rate\n",
        "GAMMA = 0.95 # aka the discount rate\n",
        "# MAX_EPISODES = 100  # optional, also defined in env setup above"
      ],
      "metadata": {
        "id": "VcUC-r6wntDW"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploration vs. Exploitation parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.001            # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "FaxgXa-dqAO5"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now it is time to dive into the training / Q-Table update methodology.<br />\n",
        "First we will define some functions needed for training phase\n",
        "\n",
        "* epsilon_greedy_action_selection: Is used to implement the epsilon greedy action selection routine.\n",
        "* compute_next_q_value: Computes the next Q-Values according to the formula from the lecture\n",
        "* reduce_epsilon: Reduces the $\\epsilon$ used for the epsilon greedy algorithm"
      ],
      "metadata": {
        "id": "6uGxo37OrNaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FUNCTION TO SELECT AN ACTION**\n",
        "\n",
        "If we simply always select the argmax() qtable value during training, we'll most likely get stuck in an explotation loop, so we'll use a random value to randomly select an action from time to time, helping the model explore , rather than exploit."
      ],
      "metadata": {
        "id": "418_yZFsrbG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
        "    '''\n",
        "    Returns an action for the agent. Note how it uses a random number to decide on\n",
        "    exploration versus explotation trade-off.\n",
        "    '''\n",
        "    random_number = np.random.random()\n",
        "    \n",
        "    # EXPLOITATION, USE BEST Q(s,a) Value\n",
        "    if random_number > epsilon:\n",
        "        # Action row for a particular state\n",
        "        state_row = q_table[discrete_state,:]    # This take a particular row \n",
        "                                                #and all the element in the coloumn\n",
        "        # Index of highest action for state\n",
        "        # Recall action is mapped to index (e.g. 0=LEFT, 1=DOWN, etc..)\n",
        "        action = np.argmax(state_row)             # This return the index of the \n",
        "                                                  # maximum element\n",
        "    \n",
        "    # EXPLORATION, USE A RANDOM ACTION\n",
        "    else:\n",
        "        # Return a random 0,1,2,3 action\n",
        "        action = env.action_space.sample()\n",
        "        \n",
        "    return action"
      ],
      "metadata": {
        "id": "PD9_ElDG4Swz"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FUNCTION FOR Q_VALUE COMPUTATION**\n",
        "\n",
        "**Here we have our main Q-Learning update equation, note how it takes in the old q-value, the next optimal q value, along with our current reward, and then updates the next q value accordingly.**"
      ],
      "metadata": {
        "id": "kfoO-Me84kOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
        "    \n",
        "    return old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)"
      ],
      "metadata": {
        "id": "Gds2qOuB4nI8"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FUNCTION TO REDUCE EPSILON**\n",
        "\n",
        "**As training continues, we need to balance explotation versus exploration, we want ot make sure our agent doesn't get trapped in a cycle going from an F square to another F Square back and forth. We also don't want our agent permanently choosing random values. We'll use the function below to try to balance this.**"
      ],
      "metadata": {
        "id": "c_aRJ-YyB1bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_epsilon(epsilon,epoch):\n",
        "    \n",
        "    return min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*epoch)"
      ],
      "metadata": {
        "id": "RBd7DgRw5R9j"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# Play 20k games\n",
        "for episode in range(EPOCHS):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    while not done:\n",
        "        action = epsilon_greedy_action_selection(epsilon,q_table, state)\n",
        "\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        \n",
        "        # Look up current/old qtable value Q(s_t,a_t)\n",
        "        old_q_value =  q_table[state,action]  \n",
        "\n",
        "        # Get the next optimal Q-Value\n",
        "        next_optimal_q_value = np.max(q_table[new_state, :])  #p.max is different \n",
        "                                  # from argmax, np.max returns the highest value \n",
        "                                  # and argmax returns the index of the highest\n",
        "                                  # value\n",
        "\n",
        "        # Compute next q value\n",
        "        next_q = compute_next_q_value(old_q_value, reward, next_optimal_q_value)   \n",
        "\n",
        "        # Update Q Table\n",
        "        q_table[state,action] = next_q\n",
        "\n",
        "        \n",
        "        \n",
        "        total_rewards = total_rewards + reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "\n",
        "\n",
        "        # print(f\"Epsilon is currently {epsilon}\")\n",
        "        # print(f\"Currently on Epoch {episode}, total reward earned so far is {total_rewards}\")\n",
        "        # print(f\"Current state {state} and performing action {action}\")\n",
        "        # #env.render()\n",
        "        # print(q_table)\n",
        "        # #time.sleep(0.01)\n",
        "        # clear_output(wait=True)\n",
        "\n",
        "        \n",
        "    episode += 1\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = reduce_epsilon(epsilon,episode) \n",
        "    rewards.append(total_rewards)\n",
        "    \n",
        "\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "tR1gkFK6B7uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now our agent can learn from our Q table**"
      ],
      "metadata": {
        "id": "BvUk8DjcPVX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(EPOCHS),np.cumsum(rewards))"
      ],
      "metadata": {
        "id": "dNJfyPNSjTD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "state = env.reset()\n",
        "rewards = 0\n",
        "for _ in range(100):\n",
        "#  env.render()\n",
        "    \n",
        "  action = np.argmax(q_table[state])  # and chose action from the Q-Table\n",
        "  state, reward, done, info = env.step(action) # Finally perform the action\n",
        "\n",
        "  time.sleep(1)\n",
        "  clear_output(wait=True)\n",
        "  env.render()\n",
        "    \n",
        "  if done:\n",
        "      break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "U9VFQjlWPhwP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}