{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPn2M/2DMeWFmlAYTbXQC9e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekpenyong-Esu/Deep-RL-Exercise/blob/main/gym_mountain_car.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!apt update && apt install xvfb\n",
        "#!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "c4D4CrxA3ye6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "Mdq_SVMJ4BDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import gym\n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iw2RIq91LB5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MountainCar-v0\")"
      ],
      "metadata": {
        "id": "KFhlHn0EH9Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK: Write a function to create a numpy array holding the bins for the observations of the car (position and velocity).** <br />\n",
        "Feel free to explore different bins per observation spacings.\n",
        "The function should take one argument which acts as the bins per observation <br />\n",
        "Hint: You can find the observations here: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
        "<br /> Hint: You will probably need around 25 bins for good results, but feel free to use less to reduce training time. <br />\n"
      ],
      "metadata": {
        "id": "fNQ36_PAIUNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the bins \n",
        "\n",
        "def create_bins(num_bins_per_observation):\n",
        "    car_position = np.linspace(-1.2, 0.6, num_bins_per_observation)  # bins for the car position\n",
        "    car_velocity = np.linspace(-0.07, 0.07, num_bins_per_observation)  # bins for the car velocity\n",
        "    bins = np.array([car_position, car_velocity])  # merge them\n",
        "    return bins"
      ],
      "metadata": {
        "id": "hULeB4G1IVGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BINS = 40  #  number of bins for this task\n",
        "BINS = create_bins(NUM_BINS)  # Create the bins used for the rest of the notebook"
      ],
      "metadata": {
        "id": "B-n3mNpTIxAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need the code to discretize the observations. We can use the same code as used in the last notebook"
      ],
      "metadata": {
        "id": "D7HlsUFTI-xL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dicretize the observation space \n",
        "def discretize_observation(observations, bins):\n",
        "    binned_observations = []\n",
        "    for i, observation in enumerate(observations):\n",
        "        discretized_observation = np.digitize(observation, bins[i])\n",
        "        binned_observations.append(discretized_observation)\n",
        "    return tuple(binned_observations) # Important for later indexing"
      ],
      "metadata": {
        "id": "syn7zTu4I_rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pWPB32f_JdEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the bin \n",
        "\n",
        "test_bins = create_bins(5)\n",
        "np.testing.assert_almost_equal(test_bins[0], [-1.2 , -0.75, -0.3 ,  0.15,  0.6])\n",
        "np.testing.assert_almost_equal(test_bins[1], [-0.07 , -0.035,  0.   ,  0.035,  0.07 ])\n",
        "\n",
        "test_observation = np.array([-0.9, 0.03])\n",
        "discretized_test_bins = discretize_observation(test_observation, test_bins)\n",
        "assert discretized_test_bins == (1, 3)"
      ],
      "metadata": {
        "id": "cETY6bZZVzV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of the q table \n",
        "\n",
        "q_table_shape = (NUM_BINS, NUM_BINS, env.action_space.n)\n",
        "q_table = np.zeros(q_table_shape)\n",
        "print(q_table.shape)"
      ],
      "metadata": {
        "id": "TVR-fxaIWGLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#epsilon greedy function\n",
        "\n",
        "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
        "    if np.random.random() > epsilon:\n",
        "        action = np.argmax(q_table[discrete_state])\n",
        "    else:\n",
        "        action = np.random.randint(0, env.action_space.n)\n",
        "    return action"
      ],
      "metadata": {
        "id": "xhFhL4tFXawi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper parameter\n",
        "\n",
        "EPOCHS = 30000\n",
        "BURN_IN = 100\n",
        "epsilon = 1\n",
        "\n",
        "EPSILON_END= 10000\n",
        "EPSILON_REDUCE = 0.0001 #epsilon / EPOCHS\n",
        "\n",
        "ALPHA = 0.8\n",
        "GAMMA = 0.9"
      ],
      "metadata": {
        "id": "m4TePAGXZmOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK: Fill out the function to compute the next Q value.**"
      ],
      "metadata": {
        "id": "OMQ8AkldZ55_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
        "    \n",
        "    return old_q_value +  ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)\n"
      ],
      "metadata": {
        "id": "PeELqhnCZreI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK: Create a function to reduce epsilon, feel free to choose any reduction method you want. We'll use a reduction with BURN_IN and EPSILON_END limits in the solution. We'll also show a way to reduce epsilon based on the number of epochs. Feel free to experiment here.**"
      ],
      "metadata": {
        "id": "Ihj9G9qnaNkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_epsilon(epsilon, epoch):\n",
        "    if BURN_IN <= epoch <= EPSILON_END:\n",
        "        epsilon-= EPSILON_REDUCE\n",
        "    return epsilon"
      ],
      "metadata": {
        "id": "alZLncWTaOfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = []  # store the epoch for plotting\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    ################################# TODO ######################################\n",
        "    \n",
        "    # TODO: Get initial observation and discretize them. Set done to False\n",
        "    initial_state = env.reset()  # get the initial observation\n",
        "    discretized_state = discretize_observation(initial_state, BINS)  # map the observation to the bins\n",
        "    done = False  # to stop current run when the car reaches the top or the time limit is reached\n",
        "    \n",
        "   \n",
        "    epochs.append(epoch)\n",
        "    \n",
        "    # TODO: As long as current run is alive (i.e not done) perform the following steps:\n",
        "    while not done:  # Perform current run as long as done is False (as long as there is still time to reach the top)\n",
        "\n",
        "        if epoch % 10000 == 0:\n",
        "          print(f\"EPOCH is {epoch}  \")\n",
        "        # TODO: Select action according to epsilon-greedy strategy\n",
        "        action = epsilon_greedy_action_selection(epsilon, q_table, discretized_state)  # Epsilon-Greedy Action Selection\n",
        "        \n",
        "        # TODO: Perform selected action and get next state. Do not forget to discretize it\n",
        "        next_state, reward, done, info = env.step(action)  # perform action and get next state\n",
        "        position, velocity = next_state\n",
        "        next_state_discretized = discretize_observation(next_state, BINS)  # map the next observation to the bins\n",
        "        \n",
        "        # TODO: Get old Q-value from Q-Table and get next optimal Q-Value\n",
        "        old_q_value =  q_table[discretized_state + (action,)]  # get the old Q-Value from the Q-Table\n",
        "        next_optimal_q_value = np.max(q_table[next_state_discretized])  # Get the next optimal Q-Value\n",
        "        \n",
        "        # TODO: Compute next Q-Value and insert it into the table\n",
        "        next_q = compute_next_q_value(old_q_value, reward, next_optimal_q_value)  # Compute next Q-Value\n",
        "        q_table[discretized_state + (action,)] = next_q  # Insert next Q-Value into the table\n",
        "        \n",
        "        # TODO: Update the old state with the new one\n",
        "        discretized_state = next_state_discretized  # Update the old state with the new one\n",
        "        \n",
        "       \n",
        "    # TODO: Reduce epsilon\n",
        "    epsilon = reduce_epsilon(epsilon, epoch)  # Reduce epsilon\n",
        "    ##############################################################################"
      ],
      "metadata": {
        "id": "d703x7MUlM-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation = env.reset()\n",
        "for counter in range(3000):\n",
        "\n",
        "    env.render()\n",
        "    display.clear_output(wait=True)\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    cv2_imshow(frame)\n",
        "    cv2.waitKey(1)\n",
        "    \n",
        "    # TODO: Get discretized observation\n",
        "    discrete_state = discretize_observation(observation, BINS)  # Get discretized observation\n",
        "    \n",
        "    # TODO: Chose action from Q-Table\n",
        "    action = np.argmax(q_table[discrete_state])  # and chose action from the Q-Table\n",
        "    \n",
        "    # TODO: Perform the action \n",
        "    observation, reward, done, info = env.step(action) # Finally perform the action\n",
        "    \n",
        "    if done:\n",
        "        print(f\"done\")\n",
        "        break\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "GKYvzEH13KlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}